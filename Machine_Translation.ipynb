{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, GRU, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "p8eJy9GXyL8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Data"
      ],
      "metadata": {
        "id": "MHQVL67vvixZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mark_start = 'sos '\n",
        "mark_end = ' eos'\n",
        "\n",
        "def load_data(file_path):\n",
        "    data_src = []\n",
        "    data_dest = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 2:\n",
        "                data_src.append(parts[0].strip())\n",
        "                data_dest.append(mark_start + parts[1].strip() + mark_end)\n",
        "    return data_src, data_dest\n"
      ],
      "metadata": {
        "id": "Df6LoK4tvnc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizate"
      ],
      "metadata": {
        "id": "jRsNd0Pxvpl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize_sentences(sentences, num_words=None):\n",
        "    tokenizer = Tokenizer(num_words=num_words, filters='', lower=True, oov_token='<unk>')\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    return tokenizer, sequences\n",
        "\n",
        "# Load data\n",
        "file_path = \"/content/ara.txt\"\n",
        "data_src, data_dest = load_data(file_path)\n",
        "\n",
        "num_words = 10000\n",
        "english_tokenizer, english_sequences = tokenize_sentences(data_src, num_words=num_words)\n",
        "arabic_tokenizer, arabic_sequences = tokenize_sentences(data_dest, num_words=num_words)\n",
        "\n",
        "\n",
        "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
        "arabic_vocab_size = len(arabic_tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "0jbhw4ZGwRJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pad sequences"
      ],
      "metadata": {
        "id": "GSJpgJzYwa0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_english_len = max(len(seq) for seq in english_sequences)\n",
        "max_arabic_len = max(len(seq) for seq in arabic_sequences)\n",
        "\n",
        "english_sequences = pad_sequences(english_sequences, maxlen=max_english_len, padding='post')\n",
        "arabic_sequences = pad_sequences(arabic_sequences, maxlen=max_arabic_len, padding='post')"
      ],
      "metadata": {
        "id": "jiC14BG-wlvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare inputs and outputs and some other parameters"
      ],
      "metadata": {
        "id": "TuT2TkFrwnhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "decoder_input_data = arabic_sequences[:, :-1]\n",
        "decoder_output_data = arabic_sequences[:, 1:]\n",
        "\n",
        "embedding_size = 128\n",
        "state_size = 256\n"
      ],
      "metadata": {
        "id": "s3AUymMww7ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build the NN"
      ],
      "metadata": {
        "id": "vNde4twEw9jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Encoder\n",
        "encoder_input = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(input_dim=english_vocab_size, output_dim=embedding_size)(encoder_input)\n",
        "encoder_gru = GRU(state_size, return_state=True)\n",
        "encoder_output, state_h = encoder_gru(encoder_embedding)\n",
        "\n",
        "# Decoder\n",
        "decoder_input = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(input_dim=arabic_vocab_size, output_dim=embedding_size)(decoder_input)\n",
        "decoder_gru = GRU(state_size, return_sequences=True, return_state=True)\n",
        "decoder_output, _ = decoder_gru(decoder_embedding, initial_state=state_h)\n",
        "decoder_dense = Dense(arabic_vocab_size, activation='softmax')\n",
        "decoder_output = decoder_dense(decoder_output)\n",
        "\n",
        "\n",
        "model = Model([encoder_input, decoder_input], decoder_output)\n",
        "model.compile(optimizer=optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3),\n",
        "    ModelCheckpoint('best_model.keras', save_best_only=True)\n",
        "]\n"
      ],
      "metadata": {
        "id": "mqn9zoBbxI-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train then Translate"
      ],
      "metadata": {
        "id": "byY5I_-KxWwh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlaISI3APU4u",
        "outputId": "64a681dc-961c-48a4-833c-3e673ee4042a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m594s\u001b[0m 4s/step - accuracy: 0.8444 - loss: 3.1510 - val_accuracy: 0.8109 - val_loss: 1.3673\n",
            "Epoch 2/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m624s\u001b[0m 4s/step - accuracy: 0.9004 - loss: 0.8002 - val_accuracy: 0.8225 - val_loss: 1.3356\n",
            "Epoch 3/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m589s\u001b[0m 4s/step - accuracy: 0.9028 - loss: 0.7513 - val_accuracy: 0.8304 - val_loss: 1.3022\n",
            "Epoch 4/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m590s\u001b[0m 4s/step - accuracy: 0.9050 - loss: 0.7136 - val_accuracy: 0.8367 - val_loss: 1.2626\n",
            "Epoch 5/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 4s/step - accuracy: 0.9059 - loss: 0.6829 - val_accuracy: 0.8367 - val_loss: 1.2933\n",
            "Epoch 6/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 4s/step - accuracy: 0.9067 - loss: 0.6557 - val_accuracy: 0.8403 - val_loss: 1.2806\n",
            "Epoch 7/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m624s\u001b[0m 4s/step - accuracy: 0.9088 - loss: 0.6224 - val_accuracy: 0.8420 - val_loss: 1.2766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x78e803d8a980> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x78e803d8bba0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Translated Sentence: هل أنت بخير؟\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model.fit(\n",
        "    [english_sequences, decoder_input_data],\n",
        "    np.expand_dims(decoder_output_data, -1),\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    validation_split=0.2,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "encoder_model = Model(encoder_input, state_h)\n",
        "\n",
        "decoder_state_input = Input(shape=(state_size,))\n",
        "decoder_output, state_h = decoder_gru(decoder_embedding, initial_state=decoder_state_input)\n",
        "decoder_output = decoder_dense(decoder_output)\n",
        "decoder_model = Model([decoder_input, decoder_state_input], [decoder_output, state_h])\n",
        "def translate_to_arabic(input_sentence, encoder_model, decoder_model, english_tokenizer, arabic_tokenizer, max_english_len, max_arabic_len):\n",
        "    input_sequence = english_tokenizer.texts_to_sequences([input_sentence])\n",
        "    input_sequence = pad_sequences(input_sequence, maxlen=max_english_len, padding='post')\n",
        "\n",
        "    thought_vector = encoder_model.predict(input_sequence)\n",
        "\n",
        "    start_token = arabic_tokenizer.word_index['sos']\n",
        "    end_token = arabic_tokenizer.word_index['eos']\n",
        "\n",
        "    decoder_input = np.array([[start_token]])\n",
        "    translated_sentence = []\n",
        "\n",
        "    for _ in range(max_arabic_len - 1):\n",
        "        predictions, thought_vector = decoder_model.predict([decoder_input, thought_vector])\n",
        "        word_index = np.argmax(predictions[0, -1, :])\n",
        "\n",
        "        if word_index == end_token:\n",
        "            break\n",
        "\n",
        "        word = arabic_tokenizer.index_word.get(word_index, '')\n",
        "        translated_sentence.append(word)\n",
        "\n",
        "        decoder_input = np.array([[word_index]])\n",
        "\n",
        "    return ' '.join(translated_sentence)\n",
        "\n",
        "# Test translation\n",
        "input_sentence = \"Hello, how are you?\"\n",
        "translated_sentence = translate_to_arabic(input_sentence, encoder_model, decoder_model, english_tokenizer, arabic_tokenizer, max_english_len, max_arabic_len)\n",
        "print(\"Translated Sentence:\", translated_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"Man is a social animal – Aristotle, the legendary Greek philosopher\"\n",
        "translated_sentence = translate_to_arabic(input_sentence, encoder_model, decoder_model, english_tokenizer, arabic_tokenizer, max_english_len, max_arabic_len)\n",
        "print(\"Translated Sentence:\", translated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv0CZQeAnAG7",
        "outputId": "438fb5b7-c0e8-44ee-b518-cff004bcbcfa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Translated Sentence: <unk> <unk> <unk> <unk>\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "Translated Sentence: <unk> <unk> <unk> <unk>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"Hell is other people\"\n",
        "translated_sentence = translate_to_arabic(input_sentence, encoder_model, decoder_model, english_tokenizer, arabic_tokenizer, max_english_len, max_arabic_len)\n",
        "print(\"Translated Sentence:\", translated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOtttGpOn6N_",
        "outputId": "55b17c29-ef57-402b-a159-29d2bf31b515"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Translated Sentence: أنا أنت في غاية ؟\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}